{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1682574923775,"user":{"displayName":"Huaqiu Liu","userId":"12120849927005481050"},"user_tz":420},"id":"rqPlP8E-bR4w","outputId":"766ed12a-85e8-4bf6-ad04-034c18350b82"},"outputs":[{"output_type":"stream","name":"stdout","text":["Thu Apr 27 05:55:22 2023       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   63C    P8    11W /  70W |      0MiB / 15360MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}],"source":["gpu_info = !nvidia-smi\n","gpu_info = '\\n'.join(gpu_info)\n","if gpu_info.find('failed') >= 0:\n","  print('没找到GPU，菜单栏点击 修改->笔记本设置->硬件加速器选择GPU，然后重新执行该代码块')\n","else:\n","  print(gpu_info)"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2652,"status":"ok","timestamp":1682574926424,"user":{"displayName":"Huaqiu Liu","userId":"12120849927005481050"},"user_tz":420},"id":"7EhcCmrObspA","outputId":"7e476b42-0209-4499-e62b-7a1ca1c7c874"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":34,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1682576713790,"user":{"displayName":"Huaqiu Liu","userId":"12120849927005481050"},"user_tz":420},"id":"v4LtyfEbbDGC","outputId":"182a3f41-4fa1-4094-f5ee-4a7bc6ea7f11"},"outputs":[{"output_type":"stream","name":"stdout","text":["cuda:0\n"]}],"source":["import gym\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import numpy as np\n","import random\n","from collections import deque\n","from gym import spaces\n","from gym.utils import seeding\n","from tqdm.notebook import tqdm\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n","#device=torch.device(\"cpu\")\n","print(device)\n","\n","# Custom CartPole Environment\n","class CustomCartPoleEnv(gym.Env):\n","    def __init__(self):\n","        \n","        self.RESOURCE_TYPE_NUM=int(1)\n","        self.RESOURCE_TYPE_NAME=[\"Hardware\"]\n","        self.sight=10\n","        self.max_running_time=10\n","        self.max_using=40\n","        self.max_total_job=20\n","        #add waittime array here\n","        self.waittime=np.zeros(self.sight)\n","\n","        \n","        self.action_space = spaces.Discrete(self.sight+1)\n","        #self.observation_space = spaces.Box(low=0, high=100, shape=(self.RESOURCE_TYPE_NUM*(1+self.sight)+2*self.sight+1,), dtype=np.float32)\n","        self.observation_space = spaces.Box(low=0, high=100, shape=(self.RESOURCE_TYPE_NUM*(1+self.sight)+1*self.sight+1,), dtype=np.float32)\n","\n","        self.seed()\n","        self.reset()\n","        \n","    def reset(self):\n","        #self.state = (self.np_random.uniform(low=0, high=100, size=(self.RESOURCE_TYPE_NUM*(1+self.sight),))*[self.x_threshold,1,self.theta_threshold_radians,1])\n","        self.waittime=np.zeros(self.sight)\n","        CPU_State=np.ones(self.RESOURCE_TYPE_NUM*self.max_running_time)*100\n","        \n","        Task_State_Amount_Known=self.np_random.uniform(low=self.max_using*0.5, high=self.max_using, size=(self.RESOURCE_TYPE_NUM*(self.sight),))\n","        Task_State_Duration_Known=self.np_random.uniform(low=self.max_running_time*0.5, high=self.max_running_time, size=((self.sight),))\n","        Task_State_UnKnown=np.array([self.max_total_job])\n","        \n","        Task_State=np.append(Task_State_Amount_Known,Task_State_Duration_Known)\n","        Task_State=np.append(Task_State,Task_State_UnKnown)\n","        \n","        for n in range(len(Task_State)):\n","            Task_State[n]=int(Task_State[n])\n","            \n","        self.state=np.append(CPU_State,Task_State)  \n","\n","        \"\"\"\n","        all_job_dur=self.state[-(1+self.sight):-1]\n","        all_job_cost=self.state[self.RESOURCE_TYPE_NUM*(self.max_running_time):self.RESOURCE_TYPE_NUM*(self.max_running_time+self.sight)]\n","        all_job_dur=all_job_dur[np.argsort(all_job_cost)]\n","        all_job_cost=all_job_cost[np.argsort(all_job_cost)]\n","        self.state[-(1+self.sight):-1]=all_job_dur\n","        self.state[self.RESOURCE_TYPE_NUM*(self.max_running_time):self.RESOURCE_TYPE_NUM*(self.max_running_time+self.sight)]=all_job_cost\n","        \"\"\"\n","\n","        #self.view_state=np.append(self.state[:self.RESOURCE_TYPE_NUM],self.waittime)\n","        self.view_state=self.state[:self.RESOURCE_TYPE_NUM]\n","        self.view_state=np.append(self.view_state,self.state[self.RESOURCE_TYPE_NUM*self.max_running_time:])\n","        #print(self.view_state)\n","        return self.view_state\n","        \n","    \n","    def check_job_available(self,job_loc):\n","        job_cost=self.state[self.RESOURCE_TYPE_NUM*(self.max_running_time+job_loc):self.RESOURCE_TYPE_NUM*(self.max_running_time+job_loc+1)]\n","        job_dur=int(self.state[-(1+self.sight)+job_loc])\n","        \n","        if job_dur==101:\n","            return -1\n","        PC_rest=self.state[:self.RESOURCE_TYPE_NUM*job_dur]\n","        PC_rest_timesplit=np.split(PC_rest,[self.RESOURCE_TYPE_NUM*(n+1) for n in range(job_dur-1)])\n","\n","        for PC_R_ST in PC_rest_timesplit:\n","            if any(PC_R_ST<job_cost):\n","                return 0\n","        \n","        return 1\n","             \n","        \n","    def seed(self, seed=None):\n","        self.np_random, seed = seeding.np_random(seed)\n","        return [seed]\n","\n","    def step(self, action):\n","        assert self.action_space.contains(action), f\"{action} ({type(action)}) invalid\"\n","        \n","        done=0\n","        job_loc=action-1\n","\n","        '''\n","        #sort jobs by their cost\n","        \n","        all_job_dur=self.state[-(1+self.sight):-1]\n","        all_job_cost=self.state[self.RESOURCE_TYPE_NUM*(self.max_running_time):self.RESOURCE_TYPE_NUM*(self.max_running_time+self.sight)]\n","        all_job_dur=all_job_dur[np.argsort(all_job_cost)]\n","        all_job_cost=all_job_cost[np.argsort(all_job_cost)]\n","        self.state[-(1+self.sight):-1]=all_job_dur\n","        self.state[self.RESOURCE_TYPE_NUM*(self.max_running_time):self.RESOURCE_TYPE_NUM*(self.max_running_time+self.sight)]=all_job_cost\n","        '''\n","\n","        state = self.state\n","        job_dur=int(self.state[-(1+self.sight)+job_loc])\n","        job_cost=self.state[self.RESOURCE_TYPE_NUM*(self.max_running_time+job_loc):self.RESOURCE_TYPE_NUM*(self.max_running_time+job_loc+1)]\n","        job_cost_timesplit=np.split(job_cost,[self.RESOURCE_TYPE_NUM*(n+1) for n in range(job_dur-1)])\n","\n","\n","        PC_rest=self.state[:int(self.RESOURCE_TYPE_NUM*job_dur)]\n","        PC_rest_timesplit=np.split(PC_rest,[self.RESOURCE_TYPE_NUM*(n+1) for n in range(job_dur-1)])\n","\n","\n","        check_out=self.check_job_available(job_loc)\n","        if action==0:\n","            reward=-1\n","            self.waittime=self.waittime+1\n","            self.state[:self.RESOURCE_TYPE_NUM*self.max_running_time]=np.append(self.state[self.RESOURCE_TYPE_NUM:self.RESOURCE_TYPE_NUM*self.max_running_time],np.ones(self.RESOURCE_TYPE_NUM)*100).flatten()\n","            self.view_state[:self.RESOURCE_TYPE_NUM]=self.state[:self.RESOURCE_TYPE_NUM]\n","            #self.view_state[self.RESOURCE_TYPE_NUM:self.sight+self.RESOURCE_TYPE_NUM]=self.waittime\n","            \n","            \n","            #all_job_dur=self.state[-(1+self.sight):-1]\n","            #reward-=sum(self.waittime[all_job_dur!=101])\n","\n","        elif check_out==-1 or check_out==0:\n","            reward=-100\n","            self.waittime=self.waittime+1\n","            self.state[:self.RESOURCE_TYPE_NUM*self.max_running_time]=np.append(self.state[self.RESOURCE_TYPE_NUM:self.RESOURCE_TYPE_NUM*self.max_running_time],np.ones(self.RESOURCE_TYPE_NUM)*100).flatten()\n","            self.view_state[:self.RESOURCE_TYPE_NUM]=self.state[:self.RESOURCE_TYPE_NUM]\n","            #self.view_state[self.RESOURCE_TYPE_NUM:self.sight+self.RESOURCE_TYPE_NUM]=self.waittime\n","            \n","            #all_job_dur=self.state[-(1+self.sight):-1]\n","            #reward-=sum(self.waittime[all_job_dur!=101])\n","\n","        else:\n","            for n in range(len(PC_rest_timesplit)):\n","                PC_rest_timesplit[n]-=job_cost\n","\n","            self.waittime[job_loc]=0\n","            \n","            self.state[:self.RESOURCE_TYPE_NUM*job_dur]=np.array(PC_rest_timesplit).flatten()\n","            self.view_state[:self.RESOURCE_TYPE_NUM]=self.state[:self.RESOURCE_TYPE_NUM]\n","            \n","            if state[-1]>0:\n","                self.state[self.RESOURCE_TYPE_NUM*(self.max_running_time+job_loc):self.RESOURCE_TYPE_NUM*(self.max_running_time+job_loc+1)]=int(self.np_random.uniform(low=self.max_using*0.5, high=self.max_using, size=(self.RESOURCE_TYPE_NUM,)))\n","                self.state[-(1+self.sight)+job_loc]=int(self.np_random.uniform(low=self.max_running_time*0.5, high=self.max_running_time, size=(1,)))\n","                state[-1]-=1\n","            \n","            else:\n","                self.state[self.RESOURCE_TYPE_NUM*(self.max_running_time+job_loc):self.RESOURCE_TYPE_NUM*(self.max_running_time+job_loc+1)]=np.ones(self.RESOURCE_TYPE_NUM)*101\n","                self.state[-(1+self.sight)+job_loc]=101\n","\n","            #self.view_state=np.append(self.state[:self.RESOURCE_TYPE_NUM],self.waittime)\n","            self.view_state=self.state[:self.RESOURCE_TYPE_NUM]\n","            self.view_state=np.append(self.view_state,self.state[self.RESOURCE_TYPE_NUM*self.max_running_time:])     \n","            reward=0\n","                \n","        job_pool=self.state[self.RESOURCE_TYPE_NUM*(self.max_running_time):self.RESOURCE_TYPE_NUM*(self.max_running_time+self.sight)]\n","        PC_rest=self.state[:int(self.RESOURCE_TYPE_NUM*job_dur)]\n","        PC_rest_timesplit=np.split(PC_rest,[self.RESOURCE_TYPE_NUM*(n+1) for n in range(job_dur-1)])\n","        \n","        if all(job_pool==101):\n","            done=1\n","            for PC_time in PC_rest_timesplit:\n","                if not np.allclose(PC_time,np.array([100,100])):\n","                    reward-=1\n","                \n","\n","        done = bool(done)\n","\n","        return np.array(self.view_state), reward, done, {}\n","\n","\n","\n","    def close(self):\n","        if self.viewer is not None:\n","            self.viewer.close()\n","            self.viewer = None\n","\n","\n","\n","# DQN Model\n","class DQN(nn.Module):\n","    def __init__(self, input_dim, output_dim, hidden_dim=256):\n","        super(DQN, self).__init__()\n","\n","        # Define the 1D convolutional layer\n","        self.conv1 = nn.Conv1d(in_channels=1, out_channels=hidden_dim, kernel_size=3, stride=1)\n","        # Define the fully connected layers\n","        #self.fc1 = nn.Linear(hidden_dim * (input_dim - 2), hidden_dim)\n","        self.fc1 = nn.Linear(input_dim, hidden_dim)\n","        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n","        self.fc3 = nn.Linear(hidden_dim, output_dim)\n","        self.drop= nn.Dropout(0.1)\n","\n","    def forward(self, x):\n","        x = x.to(device)\n","        x = x.view(-1, 1, input_dim)  # Reshape the input to be compatible with the 1D convolutional layer\n","\n","        # Apply the 1D convolutional layer\n","        #x = torch.relu(self.conv1(x))\n","\n","        # Flatten the output of the convolutional layer\n","        #x = x.view(-1, (hidden_dim)*(input_dim-2))\n","        # Apply the fully connected layers\n","        x = x.view(-1, input_dim)\n","        x = torch.relu(self.fc1(x))\n","        x = self.drop(x)\n","        x = torch.relu(self.fc2(x))\n","        x = self.drop(x)\n","        return self.fc3(x)\n","\n","def optimize_model(memory, batch_size, gamma):\n","    if len(memory) < batch_size:\n","        return\n","    batch = random.sample(memory, batch_size)\n","    states, actions, rewards, next_states, dones = zip(*batch)\n","\n","    states = torch.FloatTensor(np.array(states)).to(device)  \n","    actions = torch.LongTensor(actions).unsqueeze(1).to(device)\n","    rewards = torch.FloatTensor(rewards).unsqueeze(1).to(device)\n","    next_states = torch.FloatTensor(np.array(next_states)).to(device)  \n","    dones = torch.FloatTensor(dones).unsqueeze(1).to(device)\n","\n","    current_q_values = policy_net(states).gather(1, actions)\n","    next_q_values = target_net(next_states).max(1, keepdim=True)[0].detach()\n","    target_q_values = rewards + (gamma * next_q_values * (1 - dones))\n","\n","    loss = nn.MSELoss()(current_q_values, target_q_values)\n","    optimizer.zero_grad()\n","    loss.backward()\n","    optimizer.step()\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QQT8gwTzbFvV"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","def select_action(policy_net, state, epsilon):\n","    if random.random() > epsilon:\n","        with torch.no_grad():\n","            E_reward=policy_net(torch.FloatTensor(state).to(device))\n","            return E_reward.argmax().item()\n","    else:\n","        return random.randint(0, output_dim - 1)\n","\n","def optimize_model(episode_experience, gamma):\n","    policy_net.train()\n","    total_return = 0\n","    for state, action, reward in reversed(episode_experience):\n","        total_return = reward + gamma * total_return\n","        state_tensor = torch.FloatTensor(state).to(device)\n","        action_tensor = torch.tensor([action], device=device, dtype=torch.int64)\n","        return_tensor = torch.tensor([total_return], device=device, dtype=torch.float32)\n","\n","        q_values = policy_net(state_tensor).gather(1, action_tensor.view(-1, 1)).squeeze()\n","        loss = nn.MSELoss()(q_values, return_tensor)\n","\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","env = CustomCartPoleEnv()\n","\n","input_dim = env.observation_space.shape[0]\n","output_dim = env.action_space.n\n","#print(input_dim,output_dim)\n","hidden_dim = 64\n","gamma = 0.99\n","epsilon_start = 1.0\n","epsilon_end = 0.01\n","epsilon_decay = 0.995\n","num_episodes = 1000\n","episode_line=[]\n","total_reward_line=[]\n","\n","policy_net = DQN(input_dim, output_dim, hidden_dim).to(device)\n","#policy_net.load_state_dict(torch.load(\"/content/drive/MyDrive/MYLEARNING/RL/monte_carlo_weights.pth\"))\n","optimizer = optim.Adam(policy_net.parameters(), lr=0.0001)\n","plt.ion()\n","for episode in tqdm(range(num_episodes)):\n","    state = env.reset()\n","    done = False\n","    steps = 0\n","    epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n","    episode_experience = []\n","    total_reward = 0\n","\n","    while not done:\n","        action = select_action(policy_net, state, epsilon)\n","        next_state, reward, done, _ = env.step(action)\n","        total_reward += reward\n","        episode_experience.append((state, action, reward))\n","        state = next_state\n","        steps += 1\n","\n","    optimize_model(episode_experience, gamma)\n","    episode_line.append(episode)\n","    total_reward_line.append(total_reward)\n","    print(\"\\r\",total_reward,end=\"\")\n","\n","plt.plot(episode_line,total_reward_line)\n","torch.save(policy_net.state_dict(), \"/content/drive/MyDrive/MYLEARNING/RL/monte_carlo_weights.pth\")"]},{"cell_type":"code","source":["state = env.reset()\n","done = False\n","steps = 0\n","epsilon = max(epsilon_end, epsilon_start * (epsilon_decay ** episode))\n","episode_experience = []\n","total_reward = 0\n","\n","while not done:\n","    action = select_action(policy_net, state, epsilon)\n","    next_state, reward, done, _ = env.step(action)\n","    print(\"\\nstate:\\n\",state,\"\\naction:\\n\",action)\n","    total_reward += reward\n","    state = next_state\n","    steps += 1"],"metadata":{"id":"CDHboLGcvXb7","executionInfo":{"status":"aborted","timestamp":1682574929158,"user_tz":420,"elapsed":3,"user":{"displayName":"Huaqiu Liu","userId":"12120849927005481050"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","a=np.array([1,2,3])\n","b=np.array([0,2,3])\n","c=np.allclose(a,b)\n","print(c,a+1,a[a!=2])"],"metadata":{"id":"ETf-EgfDJaey","executionInfo":{"status":"aborted","timestamp":1682574929158,"user_tz":420,"elapsed":3,"user":{"displayName":"Huaqiu Liu","userId":"12120849927005481050"}}},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyO99av5KzLPhGdYgPAzKUFY"},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}